# ğŸ–¼ï¸ CNN Model Optimization: Lessons from CIFAR-10

ë³¸ ë¬¸ì„œëŠ” CIFAR-10 ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ìŠµë“í•œ **ë”¥ëŸ¬ë‹ ëª¨ë¸ ìµœì í™” ë° ê·œì œ(Regularization) ì „ëµ**ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ” í•µì‹¬ ë¬¸ì œ ì •ì˜: Overfitting
- **ìƒí™©**: ì´ˆê¸° LeNet-5 ëª¨ë¸ ì ìš© ì‹œ, í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ê³¼ë„í•œ ì í•©ìœ¼ë¡œ ê²€ì¦ ë°ì´í„° ì •í™•ë„ê°€ ì •ì²´ë˜ëŠ” í˜„ìƒ ë°œìƒ.
- **ë¶„ì„**: ë°ì´í„° ë³µì¡ë„ì— ë¹„í•´ ëª¨ë¸ì˜ ìˆ˜ìš©ë ¥(Capacity)ì´ ë‚®ê±°ë‚˜, í•™ìŠµì„ ì œì–´í•  ì¥ì¹˜ê°€ ë¶€ì¡±í•¨.

## ğŸ› ï¸ ìµœì í™” ì „ëµ (Optimization Strategies)
1. **Batch Normalization ì ìš©**
   - ê° ì¸µì˜ ì…ë ¥ ë¶„í¬ë¥¼ ì •ê·œí™”í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ê°œì„ í•˜ê³ , ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™”(Internal Covariate Shift) ë¬¸ì œë¥¼ ì™„í™”í•¨.
2. **Dropout & L2 Regularization**
   - **Dropout (0.5)**: ë‰´ëŸ°ì˜ ì—°ê²°ì„ ë¬´ì‘ìœ„ë¡œ ìƒëµí•˜ì—¬ íŠ¹ì • ë‰´ëŸ°ì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ë‚®ì¶¤.
   - **L2 ì •ê·œí™”**: ê°€ì¤‘ì¹˜ ê°ì‡ (Weight Decay)ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ ê°’ì´ ë¹„ëŒ€í•´ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ.
3. **Hyperparameter Tuning**
   - Adam Optimizerë¥¼ ì±„íƒí•˜ê³  Learning Rate Schedulerë¥¼ í™œìš©í•˜ì—¬ ì „ì—­ ìµœì ì (Global Minimum)ì— ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ë„ë¡ ìœ ë„.

## ğŸ“ˆ ì„±ê³¼ ë° ì¸ì‚¬ì´íŠ¸
- **ê²°ê³¼**: ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê¸°ì¤€ **ì •í™•ë„($\text{Accuracy}$) 81.61%** ë‹¬ì„±.
- **ì¸ì‚¬ì´íŠ¸**: ëª¨ë¸ì˜ ê¹Šì´(Depth)ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒë³´ë‹¤, ì ì ˆí•œ ê·œì œ ê¸°ë²•ì„ í†µí•´ **í¸í–¥-ë¶„ì‚° ìƒì¶©ê´€ê³„(Bias-Variance Trade-off)**ë¥¼ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ ì‹¤ì§ˆì ì¸ ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬ì„ì„ ì²´ë“í•¨.

---
> ğŸ”— **ê´€ë ¨ í”„ë¡œì íŠ¸**: [CIFAR-10 Image Classification](https://github.com/hwan1111/study-archive/tree/main/Projects/CIFAR10-Classification)